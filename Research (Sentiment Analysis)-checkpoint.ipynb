{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3639b1e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#pip install wordcloud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ccc95e5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import string\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import string \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "import string\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde57e26",
   "metadata": {},
   "source": [
    "# Loading Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19ab49e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "base_url = 'https://www.reviews.io/company-reviews/store/www-takealot-com/'\n",
    "\n",
    "reviews = []\n",
    "ratings = []\n",
    "\n",
    "for i in range(1, 25):\n",
    "    url = base_url+str(i)\n",
    "    \n",
    "    print(f'Scaping info from page {i}')\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    time.sleep(3)\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html')\n",
    "    \n",
    "    review = soup.find_all(class_= 'Review__body u-wordBreak--wordBreak')\n",
    "    for r in review:\n",
    "        reviews.append(r.text)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420e8b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263dc52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'Reviews' : reviews})\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d739b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The score from the website\n",
    "data['scores'] = [\"1\",\"1\",\"5\",\"1\",\"1\",\"1\",\"1\",\"1\",\"2\",\"1\",\"1\",\"1\",\"4\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\n",
    "                \"1\",\"3\",\"1\",\"1\",\"5\",\"4\",\"1\",\"5\",\"1\",\"2\",\"1\",\"1\",\"1\",\"1\",\"1\",\n",
    "                 \"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"3\",\"5\",\"1\",\"5\",\"1\",\"1\",\"1\",\"2\",\"1\",\"5\",\"5\",\"1\",\"1\",\n",
    "                 \"1\",\"2\",\"1\",\"1\",\"5\",\"1\",\"1\",\"1\",\"1\",\"5\",\"5\",\"1\",\"1\",\"5\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\n",
    "                 \"1\",\"2\",\"5\",\"5\",\"1\",\"5\",\"1\",\"1\",\"1\",\"5\",\"1\",\"1\",\"5\",\"1\",\"5\",\"1\",\"1\",\"1\",\"1\",\"5\",\n",
    "                 \"1\",\"1\",\"5\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"2\",\"5\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"5\",\"2\",\"1\",\n",
    "                 \"5\",\"5\",\"1\",\"1\",\"1\",\"1\",\"5\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"5\",\"5\",\"1\",\"1\",\"1\",\n",
    "                 \"1\",\"1\",\"1\",\"1\",\"4\",\"1\",\"1\",\"1\",\"3\",\"5\",\"1\",\"1\",\"1\",\"3\",\"1\",\"1\",\"1\",\"2\",\"5\",\"1\",\n",
    "                 \"5\",\"1\",\"1\",\"1\",\"1\",\"5\",\"1\",\"1\",\"2\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"4\",\"5\",\"1\",\"1\",\n",
    "                 \"1\",\"1\",\"4\",\"5\",\"1\",\"5\",\"1\",\"1\",\"1\",\"5\",\"1\",\"2\",\"5\",\"5\",\"1\",\"1\",\"2\",\"1\",\"1\",\"2\",\n",
    "                 \"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"4\",\"1\",\"1\",\"1\",\"4\",\"1\",\"1\",\"5\",\"1\",\"5\",\"1\",\"1\",\n",
    "                 \"1\",\"1\",\"1\",\"1\",\"5\",\"1\",\"5\",\"1\",\"1\",\"1\",\"1\",\"5\",\"5\",\"1\",\"1\",\"1\",\"1\",\"1\",\"5\",\"1\",\n",
    "                 \"1\",\"2\",\"5\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"2\",\"5\",\"1\",\"1\",\"5\",\"1\",\"5\",\"1\",\"1\",\"1\",\n",
    "                 \"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"5\",\"1\",\"1\",\"5\",\"2\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\n",
    "                 \"1\",\"1\",\"5\",\"1\",\"1\",\"1\",\"1\",\"1\",\"3\",\"1\",\"1\",\"1\",\"1\",\"5\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\n",
    "                 \"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"2\",\"1\",\"1\",\"5\",\"1\",\"1\",\"5\",\"1\",\"1\",\"4\",\"3\",\"1\",\"5\",\"1\",\n",
    "                 \"5\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"3\",\"1\",\"1\",\"1\",\"5\",\"5\",\"5\",\"5\",\"1\",\n",
    "                 \"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"3\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"4\",\"5\",\"5\",\"5\",\n",
    "                 \"5\",\"4\",\"5\",\"5\",\"5\",\"2\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"3\",\"5\",\"4\",\"5\",\"5\",\"5\",\"2\",\n",
    "                 \"3\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"4\",\"5\",\"5\",\"4\",\"5\",\"5\",\"5\",\"5\",\"4\",\"5\",\"5\",\"5\",\"5\",\n",
    "                 \"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"4\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"4\",\"4\",\n",
    "                 \"5\",\"5\",\"4\",\"4\",\"5\",\"3\",\"5\",\"4\",\"5\",\"4\",\"1\",\"4\",\"5\",\"4\",\"5\",\"5\",\"4\",\"5\",\"5\",\"4\",\n",
    "                 \"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"4\",\"4\",\"5\",\"5\",\"5\",\"4\",\"5\",\"5\",\"5\",\"4\",\"5\",\"4\",\n",
    "                 \"5\",\"5\",\"5\",\"5\",\"5\",\"5\",\"4\",\"5\",\"5\",\"5\",\"5\",\"5\",\"4\",\"1\",\"5\",\"4\",\"5\",\"5\",\"5\",\"4\",\n",
    "                 \"4\",\"5\",\"4\",\"5\",\"5\"]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7023897",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop rows with neutral scores, that is scored 3\n",
    "dataset = data.drop(labels=[16,37,138,278,306,321,338,363,370,415], axis=0)\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59957012",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98316bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifying the reviews as negative or positive\n",
    "dataset['Sentiments'] = dataset['scores'].apply(lambda x: 'Positive' if int(x) >=4  else 'Negative')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d13d952",
   "metadata": {},
   "source": [
    "# Data Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fc6f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data cleaning, removing words in brackets, make text lower case\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "dataset['Lowercase'] = dataset['Reviews'].apply(lambda x: \" \".join(word.lower() for word in x.split()))\n",
    "dataset['Numbers'] = dataset['Lowercase'].str.replace('[0-9]', '', regex=True)\n",
    "dataset['Punctuation'] = dataset['Numbers'].str.replace('[^\\w\\s]', '')\n",
    "dataset['Punctuation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88e336c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668255ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset['Stop_words'] = dataset['Punctuation'].apply(lambda x: \" \".join(word.lower() for word in x.split() if word not in stop_words))\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ebaa20",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Finding other words that means nothing to the study\n",
    "other_stopwordz = ['keep', 'something', 'done', 'get', 'ordered', 'guys', 'dont', 'please', 'go','call', 'nangamso', 'one',\n",
    "                   'sure', 'came', 'sms', 'saying', 'unless', 'right', 'cut', 'doesnt',\n",
    "                   'least', 'mtn', 'vodacom', 'us', 'ive', 'could', 'days', 'small', 'sonthey', 'covid', 'ill', 'look',\n",
    "                   'welli', '19', '10', '3', 'weekend', 'tell', 'every', 'r345', 'every', '20', '15', '1000s', 'calling',\n",
    "                   '000', 'r13', 'throws', 'boxes', 'elsewhere', 'see', 'send', 'r4438', 'october', 'know', 'houde', '2200',\n",
    "                   'house', '5pm', 'thats', 'lot', 'told', 'peoples', 'also', '5', '22nd', 'made', 'isnt', 'someone','covid19',\n",
    "                   'less', 'sla', 'whats', 'lots', 'use', 'end', 'avail', '4', 'r200000', 'okay', 'dot', 'wich', 'f','bs','im',\n",
    "                   '2', 'kzn', 'iti', 'ask', 'thabani', '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31028fa5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset['Clean_dataset'] = dataset['Stop_words'].apply(lambda x: \" \".join(word.lower() for word in x.split() if word not in other_stopwordz))\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b48bb5",
   "metadata": {},
   "source": [
    "# Creating a TFIDF Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3583d5a7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "dataset = vectorizer.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cf0020",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "freq = pd.DataFrame(dataset.toarray(), columns=vectorizer.get_feature_names())\n",
    "freq.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b359d01",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7048ae43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bar graph of top words\n",
    "top_words = pd.Series(\" \".join(dataset['Clean_dataset']).split()).value_counts()[:20]\n",
    "top_words.plot(kind='bar',figsize=(8,8))\n",
    "plt.title('Top 20 words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9dddc3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Bar graph of scores from 1-5\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "ax = sns.countplot(x=dataset[\"scores\"], data=dataset, order = dataset[\"scores\"].value_counts().index)\n",
    "for p, label in zip(ax.patches, dataset[\"scores\"].value_counts()):\n",
    "    ax.annotate(label, (p.get_x()+0.25, p.get_height()+0.5))\n",
    "top_words.plot(kind='barh', figsize=(10,8),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d64e519",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#WordCloud for the entire document\n",
    "\n",
    "text = \" \".join(dataset['Clean_dataset'])\n",
    "\n",
    "word_cloud = WordCloud(\n",
    "    width=1600,\n",
    "    height=800,\n",
    "    random_state=1,\n",
    "    background_color=\"black\",\n",
    "    colormap=\"Set2\",\n",
    "    collocations=False,\n",
    "    stopwords=STOPWORDS,\n",
    ").generate(text)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20, 8), facecolor='k')\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(word_cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51d3903",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Identifying negative reviews\n",
    "li = ['1', '2'] \n",
    "neg_score = dataset[dataset.scores.isin(li)]\n",
    "print(len(neg_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377fcf3e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# WordCloud for negative reviews\n",
    "text = \" \".join(neg_score['Clean_dataset'].tolist())\n",
    "\n",
    "word_cloud = WordCloud(\n",
    "    width=1600,\n",
    "    height=800,\n",
    "    random_state=1,\n",
    "    background_color=\"black\",\n",
    "    colormap=\"Set2\",\n",
    "    collocations=False,\n",
    "    stopwords=STOPWORDS,\n",
    ").generate(text)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20, 8), facecolor='k')\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(word_cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc2b595",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Identifying positive reviews\n",
    "li = ['4', '5'] \n",
    "pos_score = dataset[dataset.scores.isin(li)]\n",
    "print(len(pos_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11f4728",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## WordCloud for positive reviews\n",
    "text = \" \".join(pos_score['Clean_dataset'].tolist())\n",
    "\n",
    "word_cloud = WordCloud(\n",
    "    width=1600,\n",
    "    height=800,\n",
    "    random_state=1,\n",
    "    background_color=\"black\",\n",
    "    colormap=\"Set2\",\n",
    "    collocations=False,\n",
    "    stopwords=STOPWORDS,\n",
    ").generate(text)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20, 8), facecolor='k')\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(word_cloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8394b5",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Split Dataset into Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471395a6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(dataset, y,\n",
    "                                                   test_size=0.3, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89826611",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272c63f8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556de979",
   "metadata": {},
   "source": [
    "# Classification of Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd82344b",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = MultinomialNB().fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2a9e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = classifier.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd4da14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "plt.figure(dpi=100)\n",
    "mat = confusion_matrix(y_test, predicted)\n",
    "sns.heatmap(mat.T, annot=True, fmt='d', cbar=False)\n",
    "plt.title(\"Confusion matrix for Naive Bayes\")\n",
    "plt.xlabel(\"True label\")\n",
    "plt.ylabel(\"Predicted label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42968719",
   "metadata": {},
   "source": [
    "# Evaluating the Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33835894",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n Accuracy :',metrics.accuracy_score(y_test, predicted))\n",
    "print('\\n Precision :',metrics.precision_score(y_test, predicted, average = 'weighted'))\n",
    "print('\\n Recall :',metrics.recall_score(y_test, predicted, average = 'weighted'))\n",
    "print('\\n Classification report :\\n',metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06aef99",
   "metadata": {},
   "source": [
    "# Testing new reviews with Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6731a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "review = np.array([\"Takealot is worse\"])\n",
    "review_vector = vectorizer.transform(review)\n",
    "print(classifier.predict(review_vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd0d1d6",
   "metadata": {},
   "source": [
    "# Classification of Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650bf9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "classifier = svm.SVC(kernel = 'linear')\n",
    "classifier.fit(x_train, y_train)\n",
    "y_predict = classifier.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b293085",
   "metadata": {},
   "source": [
    "# Evaluating SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb580da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n Classification report :\\n', classification_report(y_test, y_predict))\n",
    "print('\\n Accuracy :', accuracy_score(y_test, y_predict))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "c5dcf8f65839ffe924455b86f4d6fb2dcc57c7583f3f1465547e593774b68eba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
